<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:webfeeds="http://webfeeds.org/rss/1.0" version="2.0"><channel><title>morganherlocker</title><link>http://morganherlocker.com</link><description>website of @morganherlocker</description><item><title>Mapping Consensus</title><dc:creator>Morgan Herlocker</dc:creator><pubDate>Thu, 07 Mar 2019 08:00:00 GMT</pubDate><description><![CDATA[<p><img src="/img/consensus.png" alt=""></p>
<p>I have long been enamored with the idea of <a href="https://twitter.com/morganherlocker/status/1063148961656070144">joining and reconciling distributed datasets</a>. With the rise of machine learning in the mapping space, coupled with an ever increasing trend towards crowd sourced mapping (individuals, NGOs, and governments), finding the thread that ties these datasets together is more important than ever. There is more open data in the mapping space than anyone knows what to do with and it is being generated faster than centralized projects like OpenStreetMap are able to ingest it. For example, nearly a year ago, <a href="https://github.com/Microsoft/USBuildingFootprints">Bing released an ML derived dataset of 125 million building footprints under an ODBL license</a>, which is compatible with OpenStreetMap (UPDATE: another <a href="https://github.com/Microsoft/CanadianBuildingFootprints">12 million Canadian footprints</a> were released <a href="https://twitter.com/jharpster/status/1103739755278168065">moments after I publish this piece</a>). The data has yet to be imported due to a host of problems that arise when attempting to process so many changes at once. While maintenance burden is an oft-cited pain point, an even bigger problem is how to detect and resolve conflicts in the data. In other words, <strong><em>how do we come to consensus on our view of the world in the age of global scale sensor networks and competing entities exerting soft power through the map? Is it even possible?</em></strong></p>
<h2 id="last-edit-rule">Last Edit Rule</h2>
<h4 id="-a-b-b-">(a + b == b)</h4>
<p>In traditional centralized mapping, deriving consensus is pretty straightforward. Whoever made the last change wins, and aside from the occasional edit war, this model is powerful in its lack of ambiguity. As a community, each contributor submits to the “last edit monarchy”, because there is so much to gain by having access to an internally consistent global street map of curated data. This model starts to break down, however, when the concepts of “time” and “edit” become fuzzy. The last edit rule only works when we operate on individual human level scales for time and edit complexity, since the curation and reconciliation models require human review. When the supply side dam breaks and 100s of millions of detailed edits can be generated in a few minutes by a swarm of GPUs, the human reviewer model cannot scale to meet the volume. Whether it&#39;s buildings extracted with machine learning from aerial imagery at a rate of millions per minute, or massive street datasets generated by cars and phones collecting ambient GPS telemetry as users go about their day, the volume of useful data is exploding exponentially. How useful is the concept of “last edit” when the last edit included 10 million miles of street centerlines, and no one is quite sure how they overlap with the existing 50 million miles in OpenStreetMap meticulously mapped out over the last decade? <strong>What happens when that volume is regenerated daily?</strong></p>
<h2 id="merge">Merge</h2>
<h4 id="-a-b-c-">(a + b == c)</h4>
<p>Distributed editing of global scale street maps is an unsolved problem, but we can look to similar domains for inspiration. One area where conflict management was similar to the present state of mapping is code authoring. Before <code>git</code>, most of us used source control systems that required “checking out” a file before editing, which put a global lock on the file, almost like checking out a book from the library. This provided a simple method for avoiding conflict, in many ways analogous to the last edit rule used in mapping today. The <a href="https://tasks.hotosm.org/">HOT tasking manager</a> even uses the check out model explicitly where an editor is able to make an exclusive lock on a specific tile while adding data in a dense region. <strong>Conflicts are impossible when there is an unambiguous process for an individual to become the arbiter of truth.</strong> While arguments can still be had over data accuracy, there is a single unambiguous state that cannot be disputed at any given point in time. <strong>Git flipped this model on its head, in large part by taking on an immense responsibility: <em>detecting and fixing conflicts in competing branches of code.</em></strong></p>
<h3 id="detection">Detection</h3>
<h4 id="-a-b-">(a + b = ?)</h4>
<p>Mapping follows a similar pattern to code authoring, in that conflicts are dangerous, but not the norm. It is rare to have multiple coders working on the same lines in a file at the same time, just as it is rare to have multiple editors working on the same object in OpenStreetMap, in comparison to the average. A system that could at least detect issues, could forward most changes through, and this is precisely what makes <code>git merge</code> so powerful. This strategy is commonly used in bespoke OpenStreetMap imports, where code will be written to identify imported features that are far away from existing features, and therefore pose minimal conflict risk.</p>
<h3 id="resolution">Resolution</h3>
<h4 id="-a-b-a-b-">(a + b = a | b)</h4>
<p>When a conflict is detected in <code>git</code>, a best effort attempt will be made to resolve the two branches. If this is not possible, the merge will fail, and the user will be asked to manually decide which branch wins. I have come to believe that these failure modes are inevitable in mapping as well, requiring human intervention. The goal of a resolution strategy should then be to:</p>
<ol>
<li>flag all conflicts</li>
<li>resolve as many conflicts as possible, perhaps taking calculated risks to do so</li>
<li>push irreconcilable issues to human reviewers through a tasking system</li>
</ol>
<p>The fewer edits get pushed to a human reviewer, the more democratized our future maps will be. Evidence for this can be found in Wikipedia&#39;s use of automated vandalism detection, such as <a href="https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/ClueBot_NG">Cluebot NG</a>, as well as Wikipedia&#39;s edit ranking systems, such as <a href="https://www.mediawiki.org/wiki/ORES_review_tool">ORES</a>, which have drastically reduced the need for human review through efficiency gains. While OpenStreetMap suffers less from vandalism than Wikipedia, handling of bulk imports and automated import streams represents a similar existential resourcing challenge.</p>
<h2 id="crdt">CRDT</h2>
<h4 id="-a-b-f-a-b-">(a) + b ==  f(a, b))</h4>
<p>The problem with the strategy above is in phase 2, automated conflict resolution. <strong>Different organizations will have varying risk tolerances, quality requirements, and access to human editors with expertise to resolve complex conflicts.</strong> This means that it may be fundamentally impossible to resolve branching map datasets in a way that is satisfactory to everyone. The <a href="https://wiki.openstreetmap.org/wiki/Import/Past_Problems">vigorous debate around bulk imports in the OpenStreetMap community</a> illustrates how divergent different community’s requirements can be when it comes to this problem.</p>
<p><a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">Conflict-free replicated data types</a> (CRDT) are one potential solution to this set of issues, as suggested by the <a href="https://bits.coop/">brilliant</a> <a href="https://www.digital-democracy.org/">folks</a> working on <a href="https://peermaps.org/">peermaps</a>, a fully p2p global street map used in some <a href="https://www.digital-democracy.org/ourwork/ra/">impactful offline heavy editing scenarios in the Amazon basin</a>. The idea with the CRDT approach more broadly is to link edits together which could potentially conflict, and materialize a de-conflicted view of each feature on the fly. This materialization step would be separate from the primary data storage responsibilities of the map editing infrastructure, opening up the possibility for various resolution strategies depending on the use case. For example, an organization could weight its own editors&#39; edits more heavily or a cautious individual wary of ML derived edits could filter based on source. Default strategies would likely be provided as well, similar to the golden path git merge strategy.</p>
<p>The CRDT strategy or similar is common in certain niches of mapping, such as crowd sourced POIs or ETA modeling where conflicting data is the norm, but significant research is still needed to make this a reality for features with strict topology requirements such as street geometry or tightly packed buildings. While some may think of p2p mapping as a niche use case, decentralized mapping generally is bringing the issues potentially addressed by CRDT to the forefront for anyone interested in large scale mapping collaborations.</p>
<h2 id="next-">Next?</h2>
<p>The conflation problem is an undertaking that no single entity is capable of solving it alone. I recently joined <a href="http://sharedstreets.io/">SharedStreets</a>, an independent non profit building an open street identifier protocol. I joined SharedStreets specifically to focus on the problem of linking branching street map data by common identifier via a fuzzy linear reference system. I believe robust common reference systems are a critical first step to building a distributed global base map (<code>git</code> got off easy on this step, since code has line numbers). In addition, I&#39;m increasingly excited by possibilities involving ultra flexible routing engines that are able to adapt to the rapidly accelerating velocity of data generation, either through smart merging/tasking or development in CRDT resolution on the fly.</p>
<p><strong>Thanks for reading! Questions or comments can be emailed to <a href="mailto:morgan.herlocker@gmail.com">morgan.herlocker@gmail.com</a> or tweeted to <a href="https://twitter.com/morganherlocker">@morganherlocker</a>.</strong></p>
<hr>
<p><em>03-07-19</em></p>
]]></description></item><item><title>Oakland Tonight</title><dc:creator>Morgan Herlocker</dc:creator><pubDate>Wed, 03 Feb 2016 08:00:00 GMT</pubDate><description><![CDATA[<p>I recently moved from Washington DC to Oakland, California. To help get to know the area better, I decided to create <a href="http://oakland-tonight.com/">oakland-tonight</a>, a simple site for tracking upcoming shows and other events around Oakland.</p>
<p>The original concept was created by <a href="https://twitter.com/tmcw">@tmcw</a> in the form of <a href="http://www.macwright.org/2014/08/11/dctn.html">dctn</a>, a nightly event scraper for DC. I liked the simplicity of the idea: instead of showing a packed calendar full of events, dctn displayed only events happening that evening. <a href="https://twitter.com/drdemsyn">@drdemsyn</a> built on this idea with <a href="http://chs-tonight.com/">chs-tonight</a> -- an event scraper for Charleston, South Carolina -- with the addition of weekly event summaries for those who would benefit from a <em>little bit</em> of planning ahead. oakland-tonight is mostly a fork of chs-tonight.</p>
<h3 id="how-it-works">How it Works</h3>
<p>Similar to dctn, and even more similar to chs-tonight, oakland-tonight uses a mix of <a href="https://en.wikipedia.org/wiki/Cron">cron</a>, <a href="http://nodejs.org/">node.js</a>, and <a href="https://pages.github.com/">Github Pages</a>. A node script is periodically run that dynamically loads and runs a set of source modules; scrapers that parse structured show data from unstructured venue website calendars. This source data is then used to generate a new version of the static site, and the result is pushed to Github. This architecture works well, because the scraper can do the slowish downloading once in a while on a tiny server, while serving the static site stays lightning fast and robust with no effort on my part.</p>
<h3 id="venues">Venues</h3>
<p>Thirteen venues are being tracked to start, but around a dozen more are <a href="https://github.com/morganherlocker/oakland-tonight/issues/1">planned</a>. Most are music oriented, but I made an effort to include venues with less of a <code>booze+music</code> focus as well.</p>
<ul>
<li><a href="www.thefoxoakland.com/">The Fox Theater</a></li>
<li><a href="www.comedyoakland.com/">Comedy Oakland</a></li>
<li><a href="http://freightandsalvage.org/">Freight &amp; Salvage</a></li>
<li><a href="http://thegoldenbullbar.com/">The Golden Bull</a></li>
<li><a href="http://legionnairesaloon.com/">The Legionnaire Saloon</a></li>
<li><a href="http://museumca.org/">Oakland Museum of California</a></li>
<li><a href="http://thenewparish.com/">The New Parish</a></li>
<li><a href="http://www.thenightlightoakland.com/">The Night Light</a></li>
<li><a href="https://oaklandoctopus.org/">The Octopus Literary Salon</a></li>
<li><a href="http://www.coliseum.com/">Oracle Arena</a></li>
<li><a href="http://www.thestarryplough.com/">Starry Plough</a></li>
<li><a href="http://sudoroom.org/">SudoRoom</a></li>
<li><a href="http://yoshis.com/">Yoshi&#39;s</a></li>
</ul>
<p>The main limiting factor here is my knowledge of the lesser known venues around town (still exploring!), and the overall awful quality of many of the smaller venue&#39;s websites. A robust text summarizer and metadata extractor that works with unstructured content will be necessary to hit the long tail of indie venues.</p>
<p>If you want to report a bug, request a feature, or add a new venue let me know <a href="https://github.com/morganherlocker/oakland-tonight/issues">on Github</a> or on twitter <a href="https://twitter.com/morganherlocker">@morganherlocker</a>.</p>
<hr>
<p><em>2-4-16</em></p>
]]></description></item><item><title>Ship Logs</title><dc:creator>Morgan Herlocker</dc:creator><pubDate>Wed, 16 Sep 2015 07:00:00 GMT</pubDate><description><![CDATA[<p>This is a map of locations extracted from digitized ship logs from 1750 to 1850. The database was compiled by a number of researchers and <a href="https://pendientedemigracion.ucm.es/info/cliwoc/">was made available</a> as a resource for studying historical climate change.</p>
<iframe width='100%' height='500px' frameBorder='0' src='https://a.tiles.mapbox.com/v4/morganherlocker.nf8hfgb2/attribution,zoompan,zoomwheel,geocoder,share.html?access_token=pk.eyJ1IjoibW9yZ2FuaGVybG9ja2VyIiwiYSI6Ii1zLU4xOWMifQ.FubD68OEerk74AYCLduMZQ#2/7.4/3.2'></iframe>

<p><em><a href="https://a.tiles.mapbox.com/v4/morganherlocker.nf8hfgb2/attribution,zoompan,zoomwheel,geocoder,share.html?access_token=pk.eyJ1IjoibW9yZ2FuaGVybG9ja2VyIiwiYSI6Ii1zLU4xOWMifQ.FubD68OEerk74AYCLduMZQ#2/7.4/3.2">Full Screen</a></em></p>
<p>It is a fairly unique dataset in both breadth and depth -- it covers 100 years of pre-industrial data, and contains often amusing notes recorded by sailors. The data also contains numerous fields for various meterological measurments, and <a href="https://www.kaggle.com/domcastro/climate-data-from-ocean-ships/wars-and-fights-draft/discussion">one researcher was able to extract all mentions of naval battles</a>, including a tussle with &quot;a sea-monster of indeterminable size&quot;.</p>
<p>I have not done anything especially useful with the database yet, but I wanted to share this map of the full dataset and encourage anyone interested in climate science or maritime history to give it a look. This map was made using <a href="https://github.com/mapbox/tippecanoe">tippecanoe</a> and <a href="https://www.mapbox.com/mapbox-studio/#darwin">Mapbox Studio</a>.</p>
<hr>
<p><em>9-16-15</em></p>
]]></description></item><item><title>geotype continued</title><dc:creator>Morgan Herlocker</dc:creator><pubDate>Thu, 26 Mar 2015 07:00:00 GMT</pubDate><description><![CDATA[<p><a href="https://github.com/morganherlocker/geotype">geotype</a> now supports pipes and png image output. You can now think of geotype as an absurd <a href="http://mapnik.org/">mapnik</a> parody from an alternate dimension.</p>
<p>Because geotype pipes png data, we can send the result straight to the imgur api and get a link back. Since we are not printing out characters to the terminal, we can set the resolution much higher (displaying characters in the terminal is a lot slower than displaying pixels in an image renderer).</p>
<p><img src="/img/imgur-pipe.gif" alt=""></p>
<p>In <a href="https://gist.github.com/morganherlocker/b14e17752227b0647e8c">just 25 lines of Javascript</a>, we can even create a caching map server that integrates with Leaflet and generates tiles on the fly. Here&#39;s the DC bus system viewed in Leaflet:</p>
<p><img src="/img/geotype-server.gif" alt=""></p>
<p>Depending on how you configure geotype, tiles will dynamically render in ~.5-1 second each. This is pretty slow, but a simple script using tile-cover can be used to pre-cache all the tiles you need, so moving around the map will be lightning fast. A script to do this would look something like this:</p>
<pre><code class="lang-js">var exec = require(&#39;child_process&#39;).execSync
var cover = require(&#39;tile-cover&#39;).tiles
var dc = require(&#39;dc.json&#39;)

var zoom = 16
var tiles = cover(dc, {min_zoom: zoom, max_zoom: zoom})
tiles.forEach(function(tile){
  var img = exec(&#39;geotype ./dc.json -p -f 0 -m 2 -t &#39;+
    tile[0]+&#39;/&#39;+tile[1]+&#39;/&#39;+zoom)
  fs.writeFileSync(__dirname+&#39;/tiles/&#39;+
    tile[0]+&#39;-&#39;+tile[1]+&#39;-&#39;+tile[2]+&#39;.png&#39;,img)
})
</code></pre>
<p>Pipe these images to a static server like s3 and you have yourself a very minimal map tile server (and I mean minimal!).</p>
<h2 id="question-round-up">question round up</h2>
<h4 id="ok-so-really-why-">ok, so really.. why?</h4>
<p>I built geotype 1 part as a fun hack and 1 part looking for a quick way to visualize test results. While working on <a href="http://turfjs.org">Turf</a> and various other geographic algorithm libraries, I find myself opening up GeoJSON data in QGIS or geojson.io hundreds of times a day. I wanted a way to speed up this feedback loop.</p>
<h4 id="geotype-for-testing-">geotype for testing?</h4>
<p>I see this working in two ways:</p>
<ol>
<li><p>geotype as a quick repl renderer. You have a script and you want to see the results quickly on the fly.</p>
</li>
<li><p>A fuzzy diff tester. Geographic algorithms are notoriously difficult to test in a way that is not absurdly brittle. Using something like geotype, you might be able to save states of algorithmic output that can be compared against in the future. Facebook has experimented with comparing images for UI testing using <a href="https://github.com/facebookarchive/huxley">Huxley</a>; I think it could be reasonable to test changes in geography algorithms using a similar methodology. Tolerance control in testing is important, because having all your tests fail with unimportant changes leads to a &quot;boy who cried wolf&quot; view of tests. The opposite, &quot;everything broke and it threw an error&quot; is not sufficient for maintaining low level geographic systems.</p>
</li>
</ol>
<h4 id="what-is-tile-cover-">what is tile-cover?</h4>
<p><a href="https://github.com/mapbox/tile-cover">tile-cover</a> is the geospatial indexer that powers the Mapbox geocoder, <a href="https://github.com/mapbox/carmen">Carmen</a> (carmen does a lot more than spatial indexing, but this is one key step). It is a pure JavaScript spatial indexer that can tell you which <a href="https://msdn.microsoft.com/en-us/library/bb259689.aspx">map tiles</a> cover a geometry at a given zoom level range. This is important for looking up shapes fast in a database, and tile-cover is particularly suited for doing this across distributed systems. It steals a bunch of ideas from <a href="http://en.wikipedia.org/wiki/Scanline_rendering">video games and old CRT displays</a> so that it can be as fast or faster than comprable libraries written in C++.</p>
<h4 id="what-s-next-">what&#39;s next?</h4>
<p>geotype currently displays characters with 4 styles. Each character is either styled as a point, a line, a polygon, or null data. It would be cool to also allow for additional styling using something like <a href="https://github.com/mapbox/simplestyle-spec/tree/master/1.1.0">simplestyle-spec</a>, which is supported by tools like <a href="http://geojson.io/">geojson.io</a>, <a href="https://www.mapbox.com/mapbox.js/">Mapbox.js</a>, and the <a href="https://help.github.com/articles/mapping-geojson-files-on-github/#styling-features">Github map display</a>.</p>
<hr>
<p><em>3-26-15</em></p>
]]></description></item><item><title>geotype</title><dc:creator>Morgan Herlocker</dc:creator><pubDate>Fri, 20 Mar 2015 07:00:00 GMT</pubDate><description><![CDATA[<p><a href="https://github.com/morganherlocker/geotype">Geotype</a> is a cli tool for rendering GeoJSON from the command line. While this might seem fairly ridiculous/pointless, it can actually be quite useful for quickly seeing what sort of geography is lurking inside a GeoJSON file. While tools like <a href="http://geojson.io/">geojson.io</a> and <a href="https://github.com/mapbox/geojsonio-cli">geojson-cli</a> provide a quick way to do this in the browser, a faster feedback loop is often helpful.</p>
<p>Here&#39;s what it looks like in action. Given a GeoJSON file that contains something like this:</p>
<p><img src="/img/world-geojson.png" alt=""></p>
<p>...we can use geotype to render it like this:</p>
<pre><code class="lang-sh">geotype world.geojson
</code></pre>
<p><img src="/img/geotype1.gif" alt=""></p>
<p>Let&#39;s say we want something higher resolution, we can overzoom using the <code>--mod</code> flag.</p>
<pre><code class="lang-sh">geotype world.geojson -m 4
</code></pre>
<p><img src="/img/geotype2.gif" alt=""></p>
<p>Don&#39;t want color? Use the <code>--no-color</code> flag to render to plain old acii.</p>
<pre><code class="lang-sh">geotype world.geojson -m 4 --no-color
</code></pre>
<p><img src="/img/geotype3.gif" alt=""></p>
<p>geotype can also distinguish between points, lines, and polygons. Here&#39;s how it renders a GeoJSON FeatureCollection containing all Vermont libraries (points), highways (lines), and zipcodes (polygons) with and without color.</p>
<pre><code class="lang-sh">geotype vermont.geojson
</code></pre>
<p><img src="/img/geotype4.gif" alt=""></p>
<p>The <code>--zoom -z</code> flag allows a pixel zoom to be specified. Each pixel is equivalent to one tile at the given zoom. Let&#39;s zoom in on our Vermont file with a simple bash for loop.</p>
<pre><code class="lang-sh">for i in `seq 6 12`; do geotype vermont.geojson -z $i; done
</code></pre>
<p><img src="/img/geotype5.gif" alt=""></p>
<p>Use the <code>--bbox</code> flag to fit the ascii &quot;image&quot; to a specific bounding box, or use the <code>--tile</code> flag to fit the render to an x/y/z map tile. Let&#39;s cycle vertically across a column of tiles against our world.geojson file to try it out.</p>
<pre><code class="lang-sh">for i in `seq 0 11`; do geotype world.geojson -t 4/$i/4 -z 9; done
</code></pre>
<p><img src="/img/geotype6.gif" alt=""></p>
<p>...at a higher zoom</p>
<pre><code class="lang-sh">for i in `seq 0 11`; do geotype world.geojson -t 4/$i/4 -z 11; done
</code></pre>
<p><img src="/img/geotype7.gif" alt=""></p>
<p>##how it works</p>
<p>Geotype is 100% JavaScript, and can be used in the browser via tools like browserify. Under the hood, it uses <a href="https://github.com/mapbox/tile-cover">tile-cover</a> and <a href="https://github.com/mapbox/tilebelt/">tilebelt</a> for rasterization, along with some clever code for picking default bboxes and pixel sizes. </p>
<p>While tile-cover and tilebelt were initially built for <a href="https://github.com/mapbox/carmen">super fast indexing in node</a>, their use of scanline algorithms originally designed for graphics processing and video games make them ideal candidates for a pure JavaScript map renderer. In fact, tile-cover is significantly faster than the best of breed distributed geospatial C++ indexers currently available, and it is designed with multi-core high cpu node.js processing in mind.</p>
<p>Using <a href="">canvas</a> and <a href="http://leafletjs.com/reference.html#tilelayer-canvas">Leaflet&#39;s canvas layer</a> it would even be possible to build a pure JavaScript clientside map renderer, for instances where GL clientside rendering is not possible. In fact, using geotype, web map rendering may even be possible in pure text browsers, such as <a href="http://en.wikipedia.org/wiki/Lynx_%28web_browser%29">Lynx</a>.</p>
<p>##caveats</p>
<p>Keep in mind, this is a mad science hack. Aside from using geotype as a test diff renderer or a repl tool, I would not recommend geotype for &quot;real&quot; map rendering. WebGL has the ability to highly parallelize work across the GPU, so geotype is never going to compete with rendering times close to those of Mapnik or WebGL.</p>
<p><em>Now that that&#39;s out of the way, let&#39;s render a bunch of shit in ascii.</em></p>
<p><img src="/img/geotype8.gif" alt=""></p>
<hr>
<p><em>3-20-15</em></p>
]]></description></item></channel></rss>
